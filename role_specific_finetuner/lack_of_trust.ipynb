{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388d7f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/shivprasad.sagare/miniconda3/envs/xalign_role/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2640ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/shivprasad.sagare/miniconda3/envs/xalign_role/lib/python3.7/site-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  category=FutureWarning,\n",
      "Access denied with the following error:\n",
      "\n",
      " \tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=1VAMOTosRNcXJh_4XxpUEuB09G4AwOauT \n",
      "\n",
      "unzip:  cannot find or open plan_order.zip, plan_order.zip.zip or plan_order.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1VAMOTosRNcXJh_4XxpUEuB09G4AwOauT\n",
    "!unzip plan_order.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c67e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang='en'\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    res = []\n",
    "    with open(file_path, encoding='utf-8') as dfile:\n",
    "        for line in dfile.readlines():\n",
    "            res.append(json.loads(line.strip()))\n",
    "    return res\n",
    "\n",
    "\n",
    "filename = \"{lang}_order.jsonl\".format(lang=lang)\n",
    "\n",
    "reader  = load_jsonl(filename)\n",
    "\n",
    "\n",
    "jsonl_data = pd.DataFrame(reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f92b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jsonl_data.to_csv('en_order.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d0267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce3988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4559ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JsonL(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, isTarget = True):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.targ = isTarget\n",
    "\n",
    "    def process_facts_shiva(facts, entity):\n",
    "        \"\"\" linearizes the facts on the encoder side \"\"\"\n",
    "        triples = []\n",
    "        subject = entity\n",
    "        # triples.append([' <S> '+subject])\n",
    "\n",
    "        for triple in facts:\n",
    "            predicate = triple[0]\n",
    "            object = triple[1]\n",
    "            triples.append([' S| '+subject, ' P| '+predicate, ' O| '+object])\n",
    "\n",
    "            for quals in triple[2]:\n",
    "                qual_subject = object\n",
    "                qual_predicate = quals[0]\n",
    "                qual_object = quals[1]\n",
    "                triples.append([' S| '+qual_subject, ' P| '+qual_predicate, ' O| '+qual_object])\n",
    "\n",
    "        return triples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        sent  = self.data.sentence[index]\n",
    "        factlist = self.data.facts[index]\n",
    "        entity = self.data.entity_name[index]\n",
    "        section = self.data.native_sentence_section[index]\n",
    "        lang = self.data.lang[index]\n",
    "    \n",
    "        fact_str = self.process_facts_shiva(factlist,entity)\n",
    "        \n",
    "#         inputs = self.tokenizer.encode_plus(\n",
    "#             fact_str,\n",
    "#             None,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=self.max_len,\n",
    "#             pad_to_max_length=True,\n",
    "#             return_token_type_ids=True,\n",
    "#             truncation=True,\n",
    "#         )\n",
    "#         CS_ids = inputs[\"input_ids\"]\n",
    "#         CS_mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.targ :\n",
    "            ref_order = self.data.order[index] \n",
    "\n",
    "            return {\n",
    "                \"fact_str\" : fact_str,\n",
    "                \"sent\" : sent,\n",
    "                \"ref_order\" : ref_order,\n",
    "                \"lang\" : lang\n",
    "            }\n",
    "        else :\n",
    "            return {\n",
    "                \"fact_str\" : fact_str,\n",
    "                \"sent\" : sent,\n",
    "                \"lang\" : lang\n",
    "            }\n",
    "            \n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e0e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    \n",
    "    fact_str_list = []\n",
    "    sent_list = []\n",
    "    ref_list = []\n",
    "    lang_list = []\n",
    "    \n",
    "    ref_flag = True \n",
    "    \n",
    "    try :\n",
    "        k = batch[0]['ref_order']\n",
    "    except:\n",
    "        ref_flag = False\n",
    "    \n",
    "    for i in batch :\n",
    "        fact_str_list.append(i['fact_str'])\n",
    "        sent_list.append(i['sent'])\n",
    "        lang_list.append(i['lang'])\n",
    "        \n",
    "        \n",
    "        if ref_flag :\n",
    "            ref_list.append(i['ref_order'])\n",
    "        \n",
    "    \n",
    "    if ref_flag :\n",
    "        return {'fact_str':fact_str_list,'sent' : sent_list,'ref_order':ref_list, 'lang': lang_list}\n",
    "    else:\n",
    "        return {'fact_str':fact_str_list,'sent' : sent_list, 'lang': lang_list}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7194faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b92041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"google/mt5-small\" #google/muril-base-cased, bert-base-multilingual-cased, xlm-roberta-base, sentence-transformers/LaBSE\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=\"/scratch\")\n",
    "model = AutoModel.from_pretrained(model_name,output_hidden_states=True, cache_dir=\"/scratch\")\n",
    "\n",
    "sent = \"Today is a weird day, let's see if it works \"\n",
    "facts = \"let's see if it works \"\n",
    "\n",
    "decoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "encoded = tokenizer.encode_plus(facts, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=encoded[\"input_ids\"], decoder_input_ids=decoded[\"input_ids\"],output_attentions=True)\n",
    "\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92bde91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len encoder hiddenstates 9\n",
      "len encoder input tokens 9\n",
      "len decocder hidden states 9\n",
      "len decoder input tokens 17\n",
      "cross attention torch.Size([6, 17, 9])\n",
      "decoder attention torch.Size([6, 17, 17])\n",
      "encoder attention torch.Size([1, 6, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "print (\"len encoder hiddenstates\", len(output.encoder_hidden_states))\n",
    "print (\"len encoder input tokens\", len(encoded[\"input_ids\"][0]))\n",
    "print (\"len decocder hidden states\", len(output.decoder_hidden_states))\n",
    "print (\"len decoder input tokens\",len(decoded[\"input_ids\"][0]))\n",
    "print (\"cross attention\", output.cross_attentions[-1][0].shape)\n",
    "print (\"decoder attention\",output.decoder_attentions[-1][0].shape)\n",
    "print (\"encoder attention\",output.encoder_attentions[-1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d81ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small',cache_dir=\"/scratch\")\n",
    "\n",
    "\n",
    "new_tokens = ['S|', 'P|', 'O|']\n",
    "new_tokens_vocab = {}\n",
    "new_tokens_vocab['additional_special_tokens'] = []\n",
    "for idx, t in enumerate(new_tokens):\n",
    "    new_tokens_vocab['additional_special_tokens'].append(t)\n",
    "num_added_toks = tokenizer.add_special_tokens(new_tokens_vocab)\n",
    "# self.config_args.logger.critical('added %s tokens' % num_added_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73cf2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 82.0/82.0 [00:00<00:00, 39.9kB/s]\n",
      "Downloading: 100%|██████████| 4.11M/4.11M [00:04<00:00, 1.07MB/s]\n",
      "Downloading: 100%|██████████| 99.0/99.0 [00:00<00:00, 72.1kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small')\n",
    "tokenizer.add_tokens(['S|', 'P|', 'O|'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e14e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshivprasad\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home2/shivprasad.sagare/miniconda3/envs/xalign_role/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/shivprasad.sagare/stuff/xalign/temp_xalign/role_specific_finetuner/wandb/run-20220826_161527-2lhugi38</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/shivprasad/temp_xalign-role_specific_finetuner/runs/2lhugi38\" target=\"_blank\">feasible-sound-94</a></strong> to <a href=\"https://wandb.ai/shivprasad/temp_xalign-role_specific_finetuner\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-2w17e6wd:v0, 3444.44MB. 1 files... Done. 0:4:23.3\n"
     ]
    }
   ],
   "source": [
    "!export WANDB_API_KEY=bf6eddaca0cddb4d9e70aa37fb5ef56202d7ef74\n",
    "\n",
    "import wandb\n",
    "\n",
    "run = wandb.init()\n",
    "\n",
    "artifact = run.use_artifact('shivprasad/xalign/model-2w17e6wd:v0', type='model')\n",
    "\n",
    "artifact_dir = artifact.download('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6602d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mt5-small were not used when initializing MT5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MT5Model were not initialized from the model checkpoint at google/mt5-small and are newly initialized: ['encoder.role_embed.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "\n",
    "model_name = \"google/mt5-small\" #google/muril-base-cased, bert-base-multilingual-cased, xlm-roberta-base, sentence-transformers/LaBSE\n",
    "PATH = \"model.ckpt\" \n",
    "\n",
    "model = AutoModel.from_pretrained(model_name,output_hidden_states=True)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "print (checkpoint.keys())\n",
    "\n",
    "new_state_dict = {}\n",
    "for ij in checkpoint['state_dict']:\n",
    "    #     print (ij[:6])\n",
    "    if ij[:6] == 'model.':\n",
    "        newkey = ij[6:]\n",
    "        new_state_dict[newkey] = checkpoint['state_dict'][ij]\n",
    "    else :\n",
    "        print (ij)\n",
    "    \n",
    "del new_state_dict['lm_head.weight']\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e2da473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode \n",
    "\n",
    "languages_map = {\n",
    "    'en': {\"label\": \"English\", 'id': 0},\n",
    "    'hi': {\"label\": \"Hindi\", 'id': 1},\n",
    "    'te': {\"label\": \"Telugu\", 'id': 2},\n",
    "    'bn': {\"label\": \"Bengali\", 'id': 3},\n",
    "    'pa': {\"label\": \"Punjabi\", 'id': 4},\n",
    "    'ur': {\"label\": \"Urdu\", 'id': 5},\n",
    "    'or': {\"label\": \"Odia\", 'id': 6},\n",
    "    'as': {\"label\": \"Assamese\", 'id': 7},\n",
    "    'gu': {\"label\": \"Gujarati\", 'id': 8},\n",
    "    'mr': {\"label\": \"Marathi\", 'id': 9},\n",
    "    'kn': {\"label\": \"Kannada\", 'id': 10},\n",
    "    'ta': {\"label\": \"Tamil\", 'id': 11},\n",
    "    'ml': {\"label\": \"Malayalam\", 'id': 12}\n",
    "}\n",
    "\n",
    "\n",
    "def get_nodes(n):\n",
    "    n = n.strip()\n",
    "    n = n.replace('(', '')\n",
    "    n = n.replace('\\\"', '')\n",
    "    n = n.replace(')', '')\n",
    "    n = n.replace(',', ' ')\n",
    "    n = n.replace('_', ' ')\n",
    "    n = unidecode.unidecode(n)\n",
    "    return n\n",
    "\n",
    "\n",
    "def get_relation(n):\n",
    "    n = n.replace('(', '')\n",
    "    n = n.replace(')', '')\n",
    "    n = n.strip()\n",
    "    n = n.split()\n",
    "    n = \"_\".join(n)\n",
    "    return n\n",
    "\n",
    "\n",
    "def linear_fact_str(fact, enable_qualifiers=True):\n",
    "    fact_str = ['<R>', get_relation(fact[0]).lower(), '<T>', get_nodes(fact[1]).lower()]\n",
    "    qualifier_str = [' '.join(['<QR>', get_relation(x[0]).lower(), '<QT>', get_nodes(x[1]).lower()]) for x in fact[2]]\n",
    "    if enable_qualifiers and len(fact[2])>0:\n",
    "        fact_str += [' '.join(qualifier_str)]\n",
    "    return fact_str\n",
    "\n",
    "def process_facts(facts, entity, section, language):\n",
    "    \"\"\" linearizes the facts on the encoder side \"\"\"\n",
    "    linearized_facts = []\n",
    "    for i in range(len(facts)):\n",
    "        linearized_facts += linear_fact_str(facts[i], enable_qualifiers=True)\n",
    "    processed_facts_str = ' '.join(linearized_facts)\n",
    "    pre_string = \"<H> %s %s <S> %s\" % (entity.lower(), processed_facts_str, section)\n",
    "#     pre_string = \"<H> %s %s \" % (entity.lower(), processed_facts_str)\n",
    "    return \"generate %s : %s\" % (languages_map[language]['label'].lower(), pre_string)\n",
    "\n",
    "\n",
    "\n",
    "def process_facts_shiva(facts, entity):\n",
    "    \"\"\" linearizes the facts on the encoder side \"\"\"\n",
    "    triples = []\n",
    "    subject = entity\n",
    "    # triples.append([' <S> '+subject])\n",
    "\n",
    "    for triple in facts:\n",
    "        predicate = triple[0]\n",
    "        object = triple[1]\n",
    "        triples.append([' S| '+subject, ' P| '+predicate, ' O| '+object])\n",
    "\n",
    "        for quals in triple[2]:\n",
    "            qual_subject = object\n",
    "            qual_predicate = quals[0]\n",
    "            qual_object = quals[1]\n",
    "            triples.append([' S| '+qual_subject, ' P| '+qual_predicate, ' O| '+qual_object])\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c677e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e846a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    \n",
    "    fact_str_list = []\n",
    "    sent_list = []\n",
    "    ref_list = []\n",
    "    lang_list = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    role_ids_list = []\n",
    "    #'role_ids': role_ids,'labels': labels.squeeze(), 'lang': torch.tensor(lang_id), 'fact_str': fact_str, 'sent': target_text, 'ref_order': ref_order\n",
    "    ref_flag = True \n",
    "    \n",
    "    try :\n",
    "        k = batch[0]['ref_order']\n",
    "    except:\n",
    "        ref_flag = False\n",
    "    \n",
    "    for i in batch :\n",
    "        fact_str_list.append(i['fact_str'])\n",
    "        sent_list.append(i['sent'])\n",
    "        lang_list.append(i['lang'])\n",
    "        input_ids_list.append(i['input_ids'])\n",
    "        attention_mask_list.append(i['attention_mask'])\n",
    "        labels_list.append(i['labels'])\n",
    "        role_ids_list.append(i['role_ids'])\n",
    "        \n",
    "        if ref_flag :\n",
    "            ref_list.append(i['ref_order'])\n",
    "        \n",
    "    \n",
    "    if ref_flag :\n",
    "        return {'fact_str':fact_str_list,'sent' : sent_list,'ref_order':ref_list, 'lang': lang_list, 'input_ids': input_ids_list, 'attention_mask': attention_mask_list, 'role_ids': role_ids_list, 'labels': labels_list}\n",
    "    else:\n",
    "        return {'fact_str':fact_str_list,'sent' : sent_list, 'lang': lang_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d66e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_source_length, max_target_length):\n",
    "        # self.df = pd.read_csv(data_path, sep='\\t', )\n",
    "        self.df = jsonl_data\n",
    "        # self.df = self.df[:len(self.df)]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.languages_map = {\n",
    "            'en': {\"label\": \"English\", 'id': 0},\n",
    "            'hi': {\"label\": \"Hindi\", 'id': 1},\n",
    "            'te': {\"label\": \"Telugu\", 'id': 2}, \n",
    "            'bn': {\"label\": \"Bengali\", 'id': 3},\n",
    "            'pa': {\"label\": \"Punjabi\", 'id': 4},\n",
    "            'ur': {\"label\": \"Urdu\", 'id': 5}, \n",
    "            'or': {\"label\": \"Odia\", 'id': 6}, \n",
    "            'as': {\"label\": \"Assamese\", 'id': 7},\n",
    "            'gu': {\"label\": \"Gujarati\", 'id': 8},\n",
    "            'mr': {\"label\": \"Marathi\", 'id': 9},\n",
    "            'kn': {\"label\": \"Kannada\", 'id': 10},\n",
    "            'ta': {\"label\": \"Tamil\", 'id': 11},\n",
    "            'ml': {\"label\": \"Malayalam\", 'id': 12} \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def process_facts_shiva(self, facts, entity):\n",
    "        \"\"\" linearizes the facts on the encoder side \"\"\"\n",
    "        triples = []\n",
    "        subject = entity\n",
    "        # print(facts)\n",
    "\n",
    "        # triples.append([' <S> '+subject])\n",
    "\n",
    "        for triple in facts:\n",
    "            predicate = triple[0]\n",
    "            object = triple[1]\n",
    "            triples.append([' S| '+subject, ' P| '+predicate, ' O| '+object])\n",
    "\n",
    "            for quals in triple[2]:\n",
    "                qual_subject = object\n",
    "                qual_predicate = quals[0]\n",
    "                qual_object = quals[1]\n",
    "                triples.append([' S| '+qual_subject, ' P| '+qual_predicate, ' O| '+qual_object])\n",
    "\n",
    "        return triples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(type(self.df.iloc[idx]['facts']))\n",
    "        # input_text = self.process_facts_shiva(eval(self.df.iloc[idx]['facts']))\n",
    "        entity = self.df.iloc[idx]['entity_name']\n",
    "        input_text = self.process_facts_shiva(self.df.iloc[idx]['facts'], entity)\n",
    "        # print(input_text, type(input_text))\n",
    "        target_text = self.df.iloc[idx]['sentence']\n",
    "        lang_code = self.df.iloc[idx]['lang']\n",
    "        lang = self.languages_map[lang_code]['label']\n",
    "        lang_id = self.languages_map[lang_code]['id']\n",
    "        prefix = f'rdf to {lang} text: '\n",
    "        # prefix = f'rdf to en text: '\n",
    "        try:\n",
    "            ref_order = self.df.iloc[idx]['order']\n",
    "        except:\n",
    "            print(\"ref_order_not _found\")\n",
    "\n",
    "        input_encoding = self.role_specific_encoding(prefix, input_text)\n",
    "        # input_encoding = self.tokenizer(input_text, return_tensors='pt', max_length=self.max_source_length ,padding='max_length', truncation=True)\n",
    "        target_encoding = self.tokenizer(target_text, return_tensors='pt', max_length=self.max_target_length ,padding='max_length', truncation=True)\n",
    "\n",
    "        input_ids, attention_mask, role_ids, fact_str = input_encoding['input_ids'], input_encoding['attention_mask'], input_encoding['role_ids'], input_encoding['fact_str']\n",
    "        labels = target_encoding['input_ids']\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100    # for ignoring the cross-entropy loss at padding locations\n",
    "\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'role_ids': role_ids,'labels': labels.squeeze(), 'lang': torch.tensor(lang_id), 'fact_str': fact_str, 'sent': target_text, 'ref_order': ref_order}   \n",
    "        # squeeze() is needed to remove the batch dimension\n",
    "\n",
    "    def role_specific_encoding(self, prefix, input_text):\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        role_ids = []\n",
    "        fact_str = ''\n",
    "\n",
    "        prefix_tokenized = self.tokenizer.encode(prefix)[:-1]   # ignoring the eos token at the end\n",
    "        input_ids.extend(prefix_tokenized)\n",
    "        attention_mask.extend([1] * len(prefix_tokenized))\n",
    "        role_ids.extend([0] * len(prefix_tokenized))\n",
    "        fact_str += prefix\n",
    "        try:\n",
    "            # data = json.loads(input_text)\n",
    "            # data = eval(input_text)\n",
    "            data = input_text\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            print(input_text)\n",
    "            raise\n",
    "\n",
    "        for triple in data:\n",
    "            subject = triple[0]\n",
    "            predicate = triple[1]\n",
    "            object = triple[2]\n",
    "\n",
    "            subject_tokenized = self.tokenizer.encode(subject)[:-1]\n",
    "            predicate_tokenized = self.tokenizer.encode(predicate)[:-1]\n",
    "            object_tokenized = self.tokenizer.encode(object)[:-1]\n",
    "\n",
    "            input_ids.extend(subject_tokenized)\n",
    "            attention_mask.extend([1] * len(subject_tokenized))\n",
    "            role_ids.extend([1] * len(subject_tokenized))\n",
    "\n",
    "            input_ids.extend(predicate_tokenized)\n",
    "            attention_mask.extend([1] * len(predicate_tokenized))\n",
    "            role_ids.extend([2] * len(predicate_tokenized))\n",
    "\n",
    "            input_ids.extend(object_tokenized)\n",
    "            attention_mask.extend([1] * len(object_tokenized))\n",
    "            role_ids.extend([3] * len(object_tokenized))\n",
    "\n",
    "            fact_str += subject + predicate + object\n",
    "        \n",
    "        input_ids.extend([self.tokenizer.eos_token_id])\n",
    "        input_ids = self.pad_and_truncate(input_ids)\n",
    "        attention_mask = self.pad_and_truncate(attention_mask)\n",
    "        role_ids = self.pad_and_truncate(role_ids)\n",
    "\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'role_ids': role_ids, 'fact_str': fact_str}\n",
    "    \n",
    "    def pad_and_truncate(self, ids):\n",
    "        if len(ids) > self.max_source_length:\n",
    "            return torch.tensor(ids[:self.max_source_length])\n",
    "        else:\n",
    "            return torch.tensor(ids + [self.tokenizer.pad_token_id] * (self.max_source_length - len(ids)))\n",
    "            # above line is hacky, i.e. padding with pad_token_id, but it works for now. for attention_mask, it should be 0.\n",
    "\n",
    "    @staticmethod\n",
    "    def create_tokenizer(tokenizer_name_or_path):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "        tokenizer.add_tokens(['S|', 'P|', 'O|'])\n",
    "        print(\"we added S|, P|, O| to the tokenizer\")\n",
    "        return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0c9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DS('en_order.csv', tokenizer, 384, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42b055e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   259,  62458,    288,   5413,   7461,    267,    259,    259, 250100,\n",
       "          58644,   1515,   6485,    259, 250101,   5255,    304,  61324,    259,\n",
       "         250102,    644,    259,   3829,  52570,    259, 250100,  58644,   1515,\n",
       "           6485,    259, 250101,    259,  96781,    259, 250102,  31729,    321,\n",
       "            259, 250100,  58644,   1515,   6485,    259, 250101,  11395,    304,\n",
       "            259, 102207,  10260,    259, 250102,  24272,      1,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'role_ids': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 1,\n",
       "         1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "         3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([19052, 58644,  1515,  6485,   261,   274, 19680,   259,  3829,  5875,\n",
       "         52570,   506,   339,   259,   262, 24272,   266, 31729,   321,   260,\n",
       "             1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]),\n",
       " 'lang': tensor(0),\n",
       " 'fact_str': 'rdf to English text:  S| Wasi Zafar P| date of birth O| 12 January 1949 S| Wasi Zafar P| occupation O| politician S| Wasi Zafar P| country of citizenship O| Pakistan',\n",
       " 'sent': 'Muhammad Wasi Zafar, (born January 12, 1949), is a Pakistani politician.',\n",
       " 'ref_order': [0, 2, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29193eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=4, num_workers=0,shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37033564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6bbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "from tqdm import tqdm\n",
    "\n",
    "ref_order = []\n",
    "pred_order = []\n",
    "\n",
    "# data_set =JsonL(jsonl_data,tokenizer,518)\n",
    "# load_params = {\"batch_size\": BATCH_SIZE, \"shuffle\": False, \"num_workers\": 0}\n",
    "# data_loader = DataLoader(data_set, **load_params, collate_fn = collate)\n",
    "\n",
    "\n",
    "for _, data in enumerate(dl, 0):\n",
    "#     print (data)\n",
    "    \n",
    "    # sent = data['sent']\n",
    "    # fact_str = data['fact_str']\n",
    "    # lang = data['lang']\n",
    "    # ref_order += data['ref_order']\n",
    "\n",
    "    # print(data)\n",
    "    # break\n",
    "    \n",
    "    # print (\"sent \",len(sent))\n",
    "    # print (\"fact \",len(fact_str))\n",
    "\n",
    "# #     print (sent)\n",
    "#     encoded_sentence = tokenizer.batch_encode_plus(sent, return_tensors=\"pt\",padding=True)\n",
    "#     encoded_facts = tokenizer.batch_encode_plus(fact_str, return_tensors=\"pt\",padding=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    print(\"model shifted to device\")\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            input_ids=torch.stack(data[\"input_ids\"]).to(device),\n",
    "            attention_mask=torch.stack(data[\"attention_mask\"]).to(device,dtype=torch.long),\n",
    "            role_ids=torch.stack(data[\"role_ids\"]).to(device,dtype=torch.long),\n",
    "            decoder_input_ids=torch.stack(data[\"labels\"]).to(device,dtype=torch.long),\n",
    "            output_attentions=True  \n",
    "        )\n",
    "    # with torch.no_grad():\n",
    "    #     output = model(\n",
    "    #         input_ids=torch.stack(data[\"input_ids\"]),\n",
    "    #         attention_mask=torch.stack(data[\"attention_mask\"]),\n",
    "    #         role_ids=torch.stack(data[\"role_ids\"]),\n",
    "    #         decoder_input_ids=torch.stack(data[\"labels\"]),\n",
    "    #         output_attentions=True  \n",
    "    #     )\n",
    "    # temp = torch.stack(data['input_ids']).to(device)\n",
    "    # print(temp)\n",
    "\n",
    "    # input_ids = data['labels']\n",
    "    # for i in range(4):\n",
    "    #     print(max(input_ids[0]))\n",
    "    # break\n",
    "    \n",
    "# #     print (output.cross_attentions)\n",
    "#     print (output.cross_attentions[-1].shape)\n",
    "#     batch_fin_crossattention = output.cross_attentions[-1].mean(dim=1)\n",
    "# #     batch_fin_crossattention = output.cross_attentions[-1][:,2,:,:]\n",
    "#     print (batch_fin_crossattention.shape)\n",
    "#     print (\"batch \",len(batch_fin_crossattention))\n",
    "    \n",
    "#     for itr in range(len(batch_fin_crossattention)):\n",
    "        \n",
    "#         fin_crossattention = batch_fin_crossattention[itr]\n",
    "#         print (fin_crossattention.shape)\n",
    "    \n",
    "#         stflag = -1\n",
    "#         currfact = -1\n",
    "#         factpos = []\n",
    "#         factpos2 = []\n",
    "        \n",
    "#         tokenized_sent = tokenizer.tokenize(sent[itr])\n",
    "#         tokenized_fact = tokenizer.tokenize(fact_str[itr])\n",
    "#         sent_len = len(tokenized_sent)\n",
    "#         for wordval in range(len(tokenized_fact)):\n",
    "\n",
    "#             tokenized_curr = tokenizer.tokenize(fact_str[itr])[wordval]\n",
    "#             if (tokenized_curr) == \"<R>\" and stflag == -1:\n",
    "#                 stflag = 1\n",
    "\n",
    "#                 sumval = 0 \n",
    "#                 fcount = 0\n",
    "#                 maxpos = 0\n",
    "#                 maxscore = -99\n",
    "#                 print (\"FIRST FACT\")\n",
    "\n",
    "#                 continue\n",
    "\n",
    "#             elif (tokenized_curr) == \"<R>\" and stflag != -1:\n",
    "#                 factpos.append(maxpos)\n",
    "#                 factpos2.append(sumval/fcount)\n",
    "\n",
    "#                 stflag = 1\n",
    "\n",
    "#                 sumval = 0 \n",
    "#                 fcount = 0\n",
    "#                 maxpos = 0\n",
    "#                 maxscore = -99\n",
    "#                 print (len(factpos) , \" FACT OVER\")\n",
    "\n",
    "#                 continue \n",
    "\n",
    "\n",
    "#             elif (tokenized_curr) == \"<QR>\":\n",
    "#     #             stflag = 0\n",
    "#                 print (\"QR FOUNDS\")\n",
    "#                 continue\n",
    "\n",
    "#             elif (tokenized_curr) == \"<S>\":\n",
    "#                 factpos.append(maxpos)\n",
    "#                 factpos2.append(sumval/fcount)\n",
    "#                 stflag = 0\n",
    "#                 print (len(factpos) , \"LAST FACT OVER\")\n",
    "#                 continue\n",
    "\n",
    "\n",
    "# #             print(fin_crossattention[:,wordval][:sent_len])\n",
    "#             max_fact = fin_crossattention[:,wordval][:sent_len].argmax()\n",
    "#             argval = fin_crossattention[:,wordval][:sent_len].max()\n",
    "#             try:\n",
    "#                 print (\"fact subword : \",tokenized_fact[wordval],end = \"\\t\\t\")\n",
    "#             except :\n",
    "#                 print (\"hello\")\n",
    "#                 pass\n",
    "            \n",
    "#             intval = np.array(max_fact.to('cpu'))\n",
    "#             tempind = encoded_facts.word_ids()[intval]\n",
    "#             print (\"matched sentence sub word : \",tokenized_sent[intval])\n",
    "\n",
    "#             if stflag == 1:\n",
    "#                 sumval += (intval+1)\n",
    "#                 fcount += 1\n",
    "#                 if argval > maxscore:\n",
    "#                     maxpos = (intval+1)\n",
    "#                     maxscore = argval\n",
    "\n",
    "\n",
    "#         print (\"===end===\")\n",
    "#         final_ordering =  np.argsort(factpos)\n",
    "#         pred_order.append(list(final_ordering))\n",
    "\n",
    "\n",
    "#         print ()\n",
    "#         print (\"works\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfc09706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d8528ea369f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_facts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#     print (len(encoded_sentence.word_ids()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ire_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ire_env/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1402\u001b[0m             )\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ire_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ire_env/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You have to initialize the model with valid token embeddings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ire_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ire_env/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ire_env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)"
     ]
    }
   ],
   "source": [
    "#ignore when gpu available\n",
    "layers = [-4, -3, -2, -1] \n",
    "\n",
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "ref_order = []\n",
    "pred_order = []\n",
    "\n",
    "count = -1\n",
    "\n",
    "for obj in tqdm(reader) :\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    #print (obj)\n",
    "    sent  = obj['sentence']\n",
    "    factlist = obj['facts']\n",
    "    ref_order.append(obj['order'])\n",
    "    entity = obj['entity_name']\n",
    "    section = obj['native_sentence_section']\n",
    "    \n",
    "    #sent_vec = get_hidden_states(sent, tokenizer, model, layers)\n",
    "    #print (sent)\n",
    "    #print (len(sent_vec))\n",
    "    \n",
    "    print(len(factlist))\n",
    "    \n",
    "    scorelist  = []\n",
    "    \n",
    "    fact_str = process_facts(factlist,entity,section,lang)\n",
    "    \n",
    "    encoded_sentence = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "    encoded_facts = tokenizer.encode_plus(fact_str, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=encoded_facts[\"input_ids\"], decoder_input_ids=encoded_sentence[\"input_ids\"],output_attentions=True)\n",
    "    \n",
    "#     print (len(encoded_sentence.word_ids()))\n",
    "#     print (encoded_sentence.word_ids())\n",
    "#     print (tokenizer.tokenize(sent))\n",
    "#     print (tokenizer.tokenize(fact_str))\n",
    "\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    print(len(output.cross_attentions))\n",
    "    print(len(output.cross_attentions[-1]))\n",
    "    print(\"-\"*30)    \n",
    "    print (output.cross_attentions[-1][0].shape)\n",
    "    \n",
    "    #using mean pooling to resolve multihead attention -> can switch to max pooling as well \n",
    "    fin_crossattention = output.cross_attentions[-1][0].max(dim=0).values\n",
    "    \n",
    "    which_fact = [] # represents the maximum alligned facts corresponding to each word of the sentence \n",
    "    \n",
    "    print (len(fin_crossattention))\n",
    "    print (fin_crossattention.shape)\n",
    "    \n",
    "#     for wordval in range(len(fin_crossattention)):\n",
    "#         #print (fin_crossattention[wordval])\n",
    "#         max_fact = fin_crossattention[wordval][:-1].argmax()\n",
    "#         try:\n",
    "#             print (\"sentence subword : \",tokenizer.tokenize(sent)[wordval],end = \"\\t\\t\")\n",
    "#         except :\n",
    "#             pass\n",
    "#         #print (np.array(max_fact))\n",
    "#         intval = np.array(max_fact)\n",
    "#         tempind = encoded_facts.word_ids()[intval]\n",
    "#         print (\"matched fact sub word : \",tokenizer.tokenize(fact_str)[intval])\n",
    "    \n",
    "    \n",
    "    stflag = -1\n",
    "    currfact = -1\n",
    "    factpos = []\n",
    "    factpos2 = []\n",
    "    \n",
    "    for wordval in range(len(fin_crossattention[0])-1):\n",
    "        \n",
    "        if (tokenizer.tokenize(fact_str)[wordval]) == \"<R>\" and stflag == -1:\n",
    "            stflag = 1\n",
    "            \n",
    "            sumval = 0 \n",
    "            fcount = 0\n",
    "            maxpos = 0\n",
    "            maxscore = -99\n",
    "            print (\"FIRST FACT\")\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        elif (tokenizer.tokenize(fact_str)[wordval]) == \"<R>\" and stflag != -1:\n",
    "            factpos.append(maxpos)\n",
    "            factpos2.append(sumval/fcount)\n",
    "            \n",
    "            stflag = 1\n",
    "            \n",
    "            sumval = 0 \n",
    "            fcount = 0\n",
    "            maxpos = 0\n",
    "            maxscore = -99\n",
    "            print (len(factpos) , \" FACT OVER\")\n",
    "            \n",
    "            continue \n",
    "            \n",
    "            \n",
    "        elif (tokenizer.tokenize(fact_str)[wordval]) == \"<QR>\":\n",
    "#             stflag = 0\n",
    "            print (\"QR FOUNDS\")\n",
    "            continue\n",
    "            \n",
    "        elif (tokenizer.tokenize(fact_str)[wordval]) == \"<S>\":\n",
    "            factpos.append(maxpos)\n",
    "            factpos2.append(sumval/fcount)\n",
    "            stflag = 0\n",
    "            print (len(factpos) , \"LAST FACT OVER\")\n",
    "            continue\n",
    "        \n",
    "            \n",
    "        max_fact = fin_crossattention[:,wordval][:-1].argmax()\n",
    "        argval = fin_crossattention[:,wordval][:-1].max()\n",
    "        try:\n",
    "            print (\"fact subword : \",tokenizer.tokenize(fact_str)[wordval],end = \"\\t\\t\")\n",
    "        except :\n",
    "            print (\"hello\")\n",
    "            pass\n",
    "        intval = np.array(max_fact)\n",
    "        tempind = encoded_facts.word_ids()[intval]\n",
    "        print (\"matched sentence sub word : \",tokenizer.tokenize(sent)[intval])\n",
    "        \n",
    "        if stflag == 1:\n",
    "            sumval += (intval+1)\n",
    "            fcount += 1\n",
    "            if argval > maxscore:\n",
    "                maxpos = (intval+1)\n",
    "                maxscore = argval\n",
    "        \n",
    "        \n",
    "    print (\"===end===\")\n",
    "    final_ordering =  np.argsort(factpos2)\n",
    "    pred_order.append(list(final_ordering))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d15eded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "qidx = random.choice([z for z in range(100)])\n",
    "print(pred_order[qidx])\n",
    "print(ref_order[qidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "588bc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(a, b):\n",
    "    assert len(a)==len(b), \"unequal size of list\"\n",
    "    count = 0\n",
    "    for y, z in zip(a, b):\n",
    "        if y==z:\n",
    "            count+=1\n",
    "    return count/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9939f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "392c35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_spearman = np.array([spearmanr(y, z)[0] for y, z in zip(pred_order, ref_order)])\n",
    "avg_kendalltau = np.array([kendalltau(y, z)[0] for y, z in zip(pred_order, ref_order)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d623540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman 0.18257142857142852\n",
      "kendalltau 0.17066666666666666\n",
      "accuracy 0.38\n"
     ]
    }
   ],
   "source": [
    "print(\"spearman\", avg_spearman.mean())\n",
    "print(\"kendalltau\", avg_kendalltau.mean())\n",
    "print(\"accuracy\", get_accuracy(ref_order, pred_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe8bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/shivprasad.sagare/miniconda3/envs/xalign_role/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620e7afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small')\n",
    "tokenizer.add_tokens(['S|', 'P|', 'O|'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1c6730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6dc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('xalign_role')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a28d93b7ba5c127d6d81a160c3b9a088dab7a0435869d0bf64a6bf7a668d824b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
